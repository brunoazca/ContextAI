
//
//  LLMManager.swift
//  Teste Context AI
//
//  Created by Bruno Azambuja Carvalho on 15/10/25.
//

import Foundation
import Combine
// MARK: - LLM Manager

@MainActor
class LLMManager: ObservableObject {
    
    @Published var isLoading = false
    @Published var lastError: String?
    @Published var lastResponse: String = ""
    
    let provider: AIModelProvider
    private let contextManager: ContextManager
    
    enum LLMError: Error, LocalizedError {
        case providerUnavailable
        
        var errorDescription: String? {
            switch self {
            case .providerUnavailable:
                return "Nenhum provedor de IA dispon√≠vel."
            }
        }
    }
    
    init(provider: AIModelProvider? = nil, contextManager: ContextManager? = nil) {
        if let provider = provider {
            self.provider = provider
        } else {
            // Usa o HybridAIProvider com Foundation Models como padr√£o
            self.provider = HybridAIProvider()
        }
        
        self.contextManager = contextManager ?? ContextManager()
    }
    
    /// M√©todo para trocar o provider do LLM
    func switchProvider(to type: HybridAIProvider.ProviderType) {
        if let hybridProvider = provider as? HybridAIProvider {
            hybridProvider.selectProvider(type)
        }
    }
    
    // MARK: - Public Methods
    
    /// Processa texto com o modelo LLM
    func processText(_ prompt: String, model: String? = nil) async throws -> String {
        isLoading = true
        lastError = nil
        
        defer {
            isLoading = false
        }
        
        guard provider.isAvailable else {
            throw LLMError.providerUnavailable
        }
        do {
            print("ü§ñ LLMManager: Enviando prompt para o provedor de IA (\(prompt.count) caracteres)")
            let responseText = try await provider.generate(prompt: prompt)
            self.lastResponse = responseText
            
            print("ü§ñ LLMManager: Resposta recebida do LLM (\(responseText.count) caracteres)")
            
            // Salva a resposta no contexto para futuras refer√™ncias
            contextManager.addContext(responseText, source: "LLM Response", metadata: [
                "model": model ?? "default",
                "prompt_length": String(prompt.count),
                "response_length": String(responseText.count)
            ])
            
            print("ü§ñ LLMManager: Resposta salva no contexto para futuras refer√™ncias")
            
            return responseText
        } catch {
            print("‚ùå LLMManager: Erro no processamento: \(error.localizedDescription)")
            self.lastError = error.localizedDescription
            throw error
        }
    }
    
    /// Processa texto reconhecido pelo OCR com contexto espec√≠fico
    func processOCRText(_ recognizedText: String) async throws -> String {
        print("ü§ñ LLMManager: Processando texto OCR com contexto hist√≥rico")
        
        // Salva o texto OCR no contexto
        contextManager.addContext(recognizedText, source: "OCR", metadata: [
            "text_length": String(recognizedText.count),
            "word_count": String(recognizedText.components(separatedBy: .whitespaces).count)
        ])
        
        // Busca contexto hist√≥rico relevante
        let historicalContext = contextManager.generateContextForLLM(currentContent: recognizedText)
        
        let prompt = """
        Analise o seguinte texto extra√≠do de uma captura de tela e forne√ßa um resumo ou an√°lise √∫til:
        
        \(historicalContext)
        
        Texto extra√≠do:
        \(recognizedText)
        
        Por favor, forne√ßa:
        1. Um resumo conciso do conte√∫do
        2. Principais pontos ou informa√ß√µes importantes
        3. Sugest√µes ou insights relevantes
        4. Relacionamento com o contexto hist√≥rico do usu√°rio (se relevante)
        
        Responda em portugu√™s brasileiro.
        """
        
        print("ü§ñ LLMManager: Enviando prompt para an√°lise com contexto hist√≥rico")
        return try await processText(prompt)
    }
    
    /// Processa texto com prompt customizado
    func processWithCustomPrompt(_ text: String, customPrompt: String) async throws -> String {
        // Salva o texto no contexto
        contextManager.addContext(text, source: "Custom Analysis", metadata: [
            "prompt_type": "custom",
            "prompt_length": String(customPrompt.count),
            "text_length": String(text.count)
        ])
        
        // Busca contexto hist√≥rico relevante
        let historicalContext = contextManager.generateContextForLLM(currentContent: text)
        
        let fullPrompt = """
        \(customPrompt)
        
        \(historicalContext)
        
        Texto para an√°lise:
        \(text)
        """
        
        return try await processText(fullPrompt)
    }
    
    /// Processa prompt customizado usando apenas o contexto hist√≥rico (sem texto adicional)
    func processCustomPromptWithContext(_ customPrompt: String) async throws -> String {
        print("ü§ñ LLMManager: Processando prompt customizado com contexto hist√≥rico (sem captura)")
        print("ü§ñ LLMManager: Prompt: \(customPrompt)")
        
        // Busca contexto hist√≥rico relevante baseado no prompt
        let historicalContext = contextManager.generateContextForLLM(currentContent: customPrompt)
        
        print("ü§ñ LLMManager: Contexto hist√≥rico encontrado: \(historicalContext.count) caracteres")
        
        let fullPrompt = """
        \(customPrompt)
        
        \(historicalContext)
        
        Use o contexto hist√≥rico acima para responder ao prompt. Se n√£o houver contexto relevante, indique isso na resposta.
        """
        
        print("ü§ñ LLMManager: Enviando prompt customizado com contexto hist√≥rico")
        return try await processText(fullPrompt)
    }
    
    // MARK: - Action Suggestion (raw text)
    /// Gera uma sugest√£o de a√ß√£o em TEXTO PURO priorizando contexto atual
    func generateSuggestionText(from recognizedText: String) async throws -> String {
        print("ü§ñ LLMManager: Gerando sugest√£o priorizando contexto atual (an√°lise a cada 15s)")
        
        // Indexa o texto OCR no contexto
        contextManager.addContext(recognizedText, source: "OCR", metadata: [
            "text_length": String(recognizedText.count),
            "for_suggestion": "true",
            "timestamp": Date().timeIntervalSince1970.description
        ])
        
        // Busca apenas as 2 entradas mais recentes (incluindo a atual)
        let recentEntries = contextManager.findRecentEntriesForSuggestions(limit: 2)
        
        var contextString = ""
        if !recentEntries.isEmpty {
            contextString = "=== CONTEXTO ATUAL (M√°ximo 2 entradas mais recentes) ===\n\n"
            for (index, entry) in recentEntries.enumerated() {
                let dateFormatter = DateFormatter()
                dateFormatter.dateStyle = .short
                dateFormatter.timeStyle = .short
                
                contextString += "[\(index + 1)] \(entry.source) - \(dateFormatter.string(from: entry.timestamp))\n"
                contextString += "\(entry.content)\n\n"
            }
            contextString += "=== FIM DO CONTEXTO ATUAL ===\n\n"
        }
        
        let prompt = """
        Voc√™ √© um assistente proativo. Com base APENAS no que o usu√°rio est√° vendo AGORA (contexto atual), escreva UMA sugest√£o objetiva, curta e diretamente acion√°vel do que voc√™ pode fazer para ajudar o usu√°rio neste momento. IGNORE contexto antigo e foque no que est√° acontecendo agora. Responda APENAS com a sugest√£o em texto (sem JSON, sem prefixos, sem porcentagens).

        \(contextString)
        
        TEXTO ATUAL (O que est√° na tela AGORA):
        \(recognizedText)
        """
        
        print("ü§ñ LLMManager: Enviando prompt para sugest√£o com contexto atual")
        let suggestion = try await processText(prompt).trimmingCharacters(in: .whitespacesAndNewlines)
        
        print("ü§ñ LLMManager: Sugest√£o gerada: \(String(suggestion.prefix(100)))...")
        
        return suggestion
    }
    
    /// Retorna o gerenciador de contexto para acesso externo
    var contextManagerInstance: ContextManager {
        return contextManager
    }
}
